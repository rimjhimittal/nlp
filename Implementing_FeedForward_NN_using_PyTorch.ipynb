{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rimjhimittal/nlp/blob/main/Implementing_FeedForward_NN_using_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-q50lzG5wyqs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 0: Preparing data**"
      ],
      "metadata": {
        "id": "Ub7nzv5nxc7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_data=torchvision.datasets.MNIST(root='./data',\n",
        "                                         train=True,\n",
        "                                         transform=torchvision.transforms.ToTensor(),\n",
        "                                         download=True\n",
        "                                         )\n",
        "test_data=torchvision.datasets.MNIST(root='./data',\n",
        "                                     train=False,\n",
        "                                     transform=torchvision.transforms.ToTensor()\n",
        "                                     )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcJZc9bvw_jM",
        "outputId": "1d0f0cbb-e12a-4d20-8977-3f4868d73d8a",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 71308016.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 29000644.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 28530081.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 4755498.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T50fXq0AylBs",
        "outputId": "fcad7e4a-9446-4ce8-8022-e64b4b7f8661",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset MNIST\n",
              "    Number of datapoints: 60000\n",
              "    Root location: ./data\n",
              "    Split: Train\n",
              "    StandardTransform\n",
              "Transform: ToTensor()"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Splitting Data in batches using DataLoader**"
      ],
      "metadata": {
        "id": "Ot8b6Jqw0Zja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader=torch.utils.data.DataLoader(dataset=training_data,\n",
        "                                         batch_size=128,\n",
        "                                         num_workers=2\n",
        "                                         )\n",
        "test_loader=torch.utils.data.DataLoader(dataset=test_data,\n",
        "                                         batch_size=128,\n",
        "                                         num_workers=2\n",
        "                                         )"
      ],
      "metadata": {
        "id": "Hyt-rvJDynzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Getting one batch**"
      ],
      "metadata": {
        "id": "ns5IoixB0M6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example=iter(train_loader)\n",
        "image,label=next(example)"
      ],
      "metadata": {
        "id": "x9NzrgO_zRrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plotting Few images of First Batch**"
      ],
      "metadata": {
        "id": "GUMR-fIs0SnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(6):\n",
        "  plt.subplot(2,3,i+1)\n",
        "  plt.imshow(image[i][0])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "eW--tao4zcFE",
        "outputId": "bd815298-9706-4366-d120-7024d5eb08e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 6 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setting Parameters**"
      ],
      "metadata": {
        "id": "C-zIkDtg0kgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs=3\n",
        "learning_rate=0.001\n",
        "input_size=784 #(28*28)\n",
        "hidden_units=500 #(hidden layer 1: number of units)\n",
        "output_size=10 #(10 unique labels for number of digits from 0-9)"
      ],
      "metadata": {
        "id": "lyD4kkwp0Hgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Defining Neural Network**"
      ],
      "metadata": {
        "id": "yk4Jmo-F1KBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "Q_UAmA-j3LgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class neural_nn(nn.Module):\n",
        "  def __init__(self,input_size,hidden_units,output_size):\n",
        "    super(neural_nn,self).__init__()\n",
        "    self.input_size=input_size\n",
        "    self.l1=nn.Linear(input_size,hidden_units)\n",
        "    self.relu=nn.ReLU()\n",
        "    self.l2=nn.Linear(hidden_units,output_size)\n",
        "  def forward(self,x):\n",
        "    out=self.l1(x)\n",
        "    out=self.relu(out)\n",
        "    out=self.l2(out)\n",
        "    return out\n",
        "model=neural_nn(input_size,hidden_units,output_size).to(device)"
      ],
      "metadata": {
        "id": "w4KMEe4O1Ja7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step2: optimizer and Loss**"
      ],
      "metadata": {
        "id": "cFu1NX9n37s1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer=torch.optim.Adam(params=model.parameters(),lr=learning_rate)\n",
        "criterion=nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "1BErrw1h3XEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training the neural network**"
      ],
      "metadata": {
        "id": "VADO1zVc4d1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "number_of_steps=len(train_loader)\n",
        "for epochs in range(num_epochs):\n",
        "  for i,(images,labels) in enumerate(train_loader):\n",
        "    images=images.reshape(-1,28*28).to(device)\n",
        "    labels=labels.to(device)\n",
        "    #Forward_pass\n",
        "    outputs=model(images)\n",
        "    loss=criterion(outputs,labels)\n",
        "    #Backward_pass and updation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print('iteration:'+str(epochs)+' step:'+str(i)+' loss:'+str(loss.item()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9hGP8Lt4c9T",
        "outputId": "b064bd28-2a24-4e04-a76c-6ff02ecdc5c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration:0 step:0 loss:0.05007172375917435\n",
            "iteration:0 step:1 loss:0.12389621138572693\n",
            "iteration:0 step:2 loss:0.03233245760202408\n",
            "iteration:0 step:3 loss:0.13977257907390594\n",
            "iteration:0 step:4 loss:0.07235335558652878\n",
            "iteration:0 step:5 loss:0.10569988936185837\n",
            "iteration:0 step:6 loss:0.0815194621682167\n",
            "iteration:0 step:7 loss:0.06794378161430359\n",
            "iteration:0 step:8 loss:0.1934235394001007\n",
            "iteration:0 step:9 loss:0.13625618815422058\n",
            "iteration:0 step:10 loss:0.11819387972354889\n",
            "iteration:0 step:11 loss:0.032713476568460464\n",
            "iteration:0 step:12 loss:0.082268126308918\n",
            "iteration:0 step:13 loss:0.047342751175165176\n",
            "iteration:0 step:14 loss:0.027989057824015617\n",
            "iteration:0 step:15 loss:0.08173156529664993\n",
            "iteration:0 step:16 loss:0.03323780372738838\n",
            "iteration:0 step:17 loss:0.03434876725077629\n",
            "iteration:0 step:18 loss:0.040842458605766296\n",
            "iteration:0 step:19 loss:0.025393398478627205\n",
            "iteration:0 step:20 loss:0.10425592958927155\n",
            "iteration:0 step:21 loss:0.08322083950042725\n",
            "iteration:0 step:22 loss:0.04091601446270943\n",
            "iteration:0 step:23 loss:0.06186981499195099\n",
            "iteration:0 step:24 loss:0.018224425613880157\n",
            "iteration:0 step:25 loss:0.040687013417482376\n",
            "iteration:0 step:26 loss:0.03445427864789963\n",
            "iteration:0 step:27 loss:0.06152250990271568\n",
            "iteration:0 step:28 loss:0.06662032008171082\n",
            "iteration:0 step:29 loss:0.0542372390627861\n",
            "iteration:0 step:30 loss:0.01897132769227028\n",
            "iteration:0 step:31 loss:0.05917877331376076\n",
            "iteration:0 step:32 loss:0.05345100164413452\n",
            "iteration:0 step:33 loss:0.06686285138130188\n",
            "iteration:0 step:34 loss:0.07452168315649033\n",
            "iteration:0 step:35 loss:0.025446461513638496\n",
            "iteration:0 step:36 loss:0.09696885198354721\n",
            "iteration:0 step:37 loss:0.02948339842259884\n",
            "iteration:0 step:38 loss:0.0638745054602623\n",
            "iteration:0 step:39 loss:0.09502808749675751\n",
            "iteration:0 step:40 loss:0.08208128809928894\n",
            "iteration:0 step:41 loss:0.07384884357452393\n",
            "iteration:0 step:42 loss:0.020356910303235054\n",
            "iteration:0 step:43 loss:0.050818245857954025\n",
            "iteration:0 step:44 loss:0.06887706369161606\n",
            "iteration:0 step:45 loss:0.08457756787538528\n",
            "iteration:0 step:46 loss:0.0616893470287323\n",
            "iteration:0 step:47 loss:0.04973644018173218\n",
            "iteration:0 step:48 loss:0.0565645769238472\n",
            "iteration:0 step:49 loss:0.02274436131119728\n",
            "iteration:0 step:50 loss:0.03707833215594292\n",
            "iteration:0 step:51 loss:0.04407733678817749\n",
            "iteration:0 step:52 loss:0.03992597758769989\n",
            "iteration:0 step:53 loss:0.16713102161884308\n",
            "iteration:0 step:54 loss:0.11953672021627426\n",
            "iteration:0 step:55 loss:0.08550141751766205\n",
            "iteration:0 step:56 loss:0.10402122884988785\n",
            "iteration:0 step:57 loss:0.07170452177524567\n",
            "iteration:0 step:58 loss:0.05217266455292702\n",
            "iteration:0 step:59 loss:0.07431244105100632\n",
            "iteration:0 step:60 loss:0.05157317593693733\n",
            "iteration:0 step:61 loss:0.07386326789855957\n",
            "iteration:0 step:62 loss:0.06112545356154442\n",
            "iteration:0 step:63 loss:0.07761038839817047\n",
            "iteration:0 step:64 loss:0.17874214053153992\n",
            "iteration:0 step:65 loss:0.03295062854886055\n",
            "iteration:0 step:66 loss:0.08598555624485016\n",
            "iteration:0 step:67 loss:0.0660289004445076\n",
            "iteration:0 step:68 loss:0.17743204534053802\n",
            "iteration:0 step:69 loss:0.14852117002010345\n",
            "iteration:0 step:70 loss:0.06674794852733612\n",
            "iteration:0 step:71 loss:0.053682468831539154\n",
            "iteration:0 step:72 loss:0.10527686774730682\n",
            "iteration:0 step:73 loss:0.07874147593975067\n",
            "iteration:0 step:74 loss:0.04725300893187523\n",
            "iteration:0 step:75 loss:0.04604446515440941\n",
            "iteration:0 step:76 loss:0.05456011742353439\n",
            "iteration:0 step:77 loss:0.041370414197444916\n",
            "iteration:0 step:78 loss:0.05868656188249588\n",
            "iteration:0 step:79 loss:0.1128932386636734\n",
            "iteration:0 step:80 loss:0.10788455605506897\n",
            "iteration:0 step:81 loss:0.027746642008423805\n",
            "iteration:0 step:82 loss:0.01148801576346159\n",
            "iteration:0 step:83 loss:0.04851497337222099\n",
            "iteration:0 step:84 loss:0.05253984034061432\n",
            "iteration:0 step:85 loss:0.06550689786672592\n",
            "iteration:0 step:86 loss:0.03959958255290985\n",
            "iteration:0 step:87 loss:0.06733769178390503\n",
            "iteration:0 step:88 loss:0.02636031061410904\n",
            "iteration:0 step:89 loss:0.035309772938489914\n",
            "iteration:0 step:90 loss:0.12280956655740738\n",
            "iteration:0 step:91 loss:0.12565994262695312\n",
            "iteration:0 step:92 loss:0.09907247126102448\n",
            "iteration:0 step:93 loss:0.09588015079498291\n",
            "iteration:0 step:94 loss:0.022801050916314125\n",
            "iteration:0 step:95 loss:0.055434808135032654\n",
            "iteration:0 step:96 loss:0.055499859154224396\n",
            "iteration:0 step:97 loss:0.06163724139332771\n",
            "iteration:0 step:98 loss:0.1932065486907959\n",
            "iteration:0 step:99 loss:0.10757170617580414\n",
            "iteration:0 step:100 loss:0.04504750296473503\n",
            "iteration:0 step:101 loss:0.07462137937545776\n",
            "iteration:0 step:102 loss:0.04491783678531647\n",
            "iteration:0 step:103 loss:0.05499159172177315\n",
            "iteration:0 step:104 loss:0.05392312631011009\n",
            "iteration:0 step:105 loss:0.05345650762319565\n",
            "iteration:0 step:106 loss:0.08917368203401566\n",
            "iteration:0 step:107 loss:0.06281810998916626\n",
            "iteration:0 step:108 loss:0.05061958357691765\n",
            "iteration:0 step:109 loss:0.11218683421611786\n",
            "iteration:0 step:110 loss:0.05404253304004669\n",
            "iteration:0 step:111 loss:0.08177026361227036\n",
            "iteration:0 step:112 loss:0.06820765137672424\n",
            "iteration:0 step:113 loss:0.07380837947130203\n",
            "iteration:0 step:114 loss:0.05554404482245445\n",
            "iteration:0 step:115 loss:0.10783691704273224\n",
            "iteration:0 step:116 loss:0.02763730101287365\n",
            "iteration:0 step:117 loss:0.04153607785701752\n",
            "iteration:0 step:118 loss:0.0879928395152092\n",
            "iteration:0 step:119 loss:0.09030363708734512\n",
            "iteration:0 step:120 loss:0.05512548238039017\n",
            "iteration:0 step:121 loss:0.05299380421638489\n",
            "iteration:0 step:122 loss:0.05254238098859787\n",
            "iteration:0 step:123 loss:0.13219335675239563\n",
            "iteration:0 step:124 loss:0.09435852617025375\n",
            "iteration:0 step:125 loss:0.07171689718961716\n",
            "iteration:0 step:126 loss:0.056390538811683655\n",
            "iteration:0 step:127 loss:0.060965828597545624\n",
            "iteration:0 step:128 loss:0.06545313447713852\n",
            "iteration:0 step:129 loss:0.05936092510819435\n",
            "iteration:0 step:130 loss:0.09251491725444794\n",
            "iteration:0 step:131 loss:0.04586612060666084\n",
            "iteration:0 step:132 loss:0.08117490261793137\n",
            "iteration:0 step:133 loss:0.08683623373508453\n",
            "iteration:0 step:134 loss:0.07997561991214752\n",
            "iteration:0 step:135 loss:0.03801639750599861\n",
            "iteration:0 step:136 loss:0.05161776393651962\n",
            "iteration:0 step:137 loss:0.05791778489947319\n",
            "iteration:0 step:138 loss:0.13296039402484894\n",
            "iteration:0 step:139 loss:0.07758571952581406\n",
            "iteration:0 step:140 loss:0.06216372549533844\n",
            "iteration:0 step:141 loss:0.05389543995261192\n",
            "iteration:0 step:142 loss:0.03261527046561241\n",
            "iteration:0 step:143 loss:0.0551733635365963\n",
            "iteration:0 step:144 loss:0.05299079790711403\n",
            "iteration:0 step:145 loss:0.05331246554851532\n",
            "iteration:0 step:146 loss:0.04133552312850952\n",
            "iteration:0 step:147 loss:0.017912164330482483\n",
            "iteration:0 step:148 loss:0.023343170061707497\n",
            "iteration:0 step:149 loss:0.08357582241296768\n",
            "iteration:0 step:150 loss:0.028112636879086494\n",
            "iteration:0 step:151 loss:0.1175350546836853\n",
            "iteration:0 step:152 loss:0.08812283724546432\n",
            "iteration:0 step:153 loss:0.03223535045981407\n",
            "iteration:0 step:154 loss:0.053334809839725494\n",
            "iteration:0 step:155 loss:0.06165961176156998\n",
            "iteration:0 step:156 loss:0.07408257573843002\n",
            "iteration:0 step:157 loss:0.06846027076244354\n",
            "iteration:0 step:158 loss:0.07461931556463242\n",
            "iteration:0 step:159 loss:0.016133716329932213\n",
            "iteration:0 step:160 loss:0.043067567050457\n",
            "iteration:0 step:161 loss:0.15244829654693604\n",
            "iteration:0 step:162 loss:0.1429079920053482\n",
            "iteration:0 step:163 loss:0.09576988965272903\n",
            "iteration:0 step:164 loss:0.055076368153095245\n",
            "iteration:0 step:165 loss:0.05089663341641426\n",
            "iteration:0 step:166 loss:0.03651803731918335\n",
            "iteration:0 step:167 loss:0.0325251929461956\n",
            "iteration:0 step:168 loss:0.12995141744613647\n",
            "iteration:0 step:169 loss:0.032940901815891266\n",
            "iteration:0 step:170 loss:0.018152400851249695\n",
            "iteration:0 step:171 loss:0.0880601778626442\n",
            "iteration:0 step:172 loss:0.047266945242881775\n",
            "iteration:0 step:173 loss:0.12590576708316803\n",
            "iteration:0 step:174 loss:0.037180110812187195\n",
            "iteration:0 step:175 loss:0.09708234667778015\n",
            "iteration:0 step:176 loss:0.11534348875284195\n",
            "iteration:0 step:177 loss:0.03999566286802292\n",
            "iteration:0 step:178 loss:0.04014131799340248\n",
            "iteration:0 step:179 loss:0.03839154168963432\n",
            "iteration:0 step:180 loss:0.052093591541051865\n",
            "iteration:0 step:181 loss:0.06711585819721222\n",
            "iteration:0 step:182 loss:0.06719598919153214\n",
            "iteration:0 step:183 loss:0.03783554583787918\n",
            "iteration:0 step:184 loss:0.10704033821821213\n",
            "iteration:0 step:185 loss:0.0668959692120552\n",
            "iteration:0 step:186 loss:0.08778025954961777\n",
            "iteration:0 step:187 loss:0.09409797191619873\n",
            "iteration:0 step:188 loss:0.03248466178774834\n",
            "iteration:0 step:189 loss:0.08006580173969269\n",
            "iteration:0 step:190 loss:0.009400058537721634\n",
            "iteration:0 step:191 loss:0.0663377195596695\n",
            "iteration:0 step:192 loss:0.04785333201289177\n",
            "iteration:0 step:193 loss:0.09298960864543915\n",
            "iteration:0 step:194 loss:0.045028284192085266\n",
            "iteration:0 step:195 loss:0.013983986340463161\n",
            "iteration:0 step:196 loss:0.08408278226852417\n",
            "iteration:0 step:197 loss:0.06811994314193726\n",
            "iteration:0 step:198 loss:0.011315166018903255\n",
            "iteration:0 step:199 loss:0.12104754149913788\n",
            "iteration:0 step:200 loss:0.058590974658727646\n",
            "iteration:0 step:201 loss:0.07715345919132233\n",
            "iteration:0 step:202 loss:0.03876076266169548\n",
            "iteration:0 step:203 loss:0.044961780309677124\n",
            "iteration:0 step:204 loss:0.02508201263844967\n",
            "iteration:0 step:205 loss:0.05853116884827614\n",
            "iteration:0 step:206 loss:0.10687607526779175\n",
            "iteration:0 step:207 loss:0.1575641930103302\n",
            "iteration:0 step:208 loss:0.15070851147174835\n",
            "iteration:0 step:209 loss:0.05997508764266968\n",
            "iteration:0 step:210 loss:0.08039984107017517\n",
            "iteration:0 step:211 loss:0.0232259389013052\n",
            "iteration:0 step:212 loss:0.1996804028749466\n",
            "iteration:0 step:213 loss:0.017455672845244408\n",
            "iteration:0 step:214 loss:0.06667669117450714\n",
            "iteration:0 step:215 loss:0.06342876702547073\n",
            "iteration:0 step:216 loss:0.06700657308101654\n",
            "iteration:0 step:217 loss:0.11495774239301682\n",
            "iteration:0 step:218 loss:0.024020887911319733\n",
            "iteration:0 step:219 loss:0.045572541654109955\n",
            "iteration:0 step:220 loss:0.05700429156422615\n",
            "iteration:0 step:221 loss:0.11195316910743713\n",
            "iteration:0 step:222 loss:0.030023669824004173\n",
            "iteration:0 step:223 loss:0.08521975576877594\n",
            "iteration:0 step:224 loss:0.0560380183160305\n",
            "iteration:0 step:225 loss:0.02979906275868416\n",
            "iteration:0 step:226 loss:0.07886645942926407\n",
            "iteration:0 step:227 loss:0.05871139466762543\n",
            "iteration:0 step:228 loss:0.07344385981559753\n",
            "iteration:0 step:229 loss:0.06563259661197662\n",
            "iteration:0 step:230 loss:0.034138184040784836\n",
            "iteration:0 step:231 loss:0.04779016599059105\n",
            "iteration:0 step:232 loss:0.04933122172951698\n",
            "iteration:0 step:233 loss:0.06943342089653015\n",
            "iteration:0 step:234 loss:0.08810923993587494\n",
            "iteration:0 step:235 loss:0.06684532761573792\n",
            "iteration:0 step:236 loss:0.03005516156554222\n",
            "iteration:0 step:237 loss:0.02584270015358925\n",
            "iteration:0 step:238 loss:0.06837157905101776\n",
            "iteration:0 step:239 loss:0.07291774451732635\n",
            "iteration:0 step:240 loss:0.05032511055469513\n",
            "iteration:0 step:241 loss:0.07685963809490204\n",
            "iteration:0 step:242 loss:0.04116940125823021\n",
            "iteration:0 step:243 loss:0.13332654535770416\n",
            "iteration:0 step:244 loss:0.07147270441055298\n",
            "iteration:0 step:245 loss:0.05230775475502014\n",
            "iteration:0 step:246 loss:0.08562453091144562\n",
            "iteration:0 step:247 loss:0.11217296868562698\n",
            "iteration:0 step:248 loss:0.07022924721240997\n",
            "iteration:0 step:249 loss:0.04333021864295006\n",
            "iteration:0 step:250 loss:0.091103196144104\n",
            "iteration:0 step:251 loss:0.057326581329107285\n",
            "iteration:0 step:252 loss:0.07100597023963928\n",
            "iteration:0 step:253 loss:0.10543076694011688\n",
            "iteration:0 step:254 loss:0.033734533935785294\n",
            "iteration:0 step:255 loss:0.07683255523443222\n",
            "iteration:0 step:256 loss:0.06629405915737152\n",
            "iteration:0 step:257 loss:0.01994677446782589\n",
            "iteration:0 step:258 loss:0.04455212876200676\n",
            "iteration:0 step:259 loss:0.04433877393603325\n",
            "iteration:0 step:260 loss:0.1002972200512886\n",
            "iteration:0 step:261 loss:0.07696493715047836\n",
            "iteration:0 step:262 loss:0.030413322150707245\n",
            "iteration:0 step:263 loss:0.07909506559371948\n",
            "iteration:0 step:264 loss:0.012980660423636436\n",
            "iteration:0 step:265 loss:0.02699623629450798\n",
            "iteration:0 step:266 loss:0.054988764226436615\n",
            "iteration:0 step:267 loss:0.01934162527322769\n",
            "iteration:0 step:268 loss:0.098452128469944\n",
            "iteration:0 step:269 loss:0.06796969473361969\n",
            "iteration:0 step:270 loss:0.0874435305595398\n",
            "iteration:0 step:271 loss:0.1155853420495987\n",
            "iteration:0 step:272 loss:0.09222368896007538\n",
            "iteration:0 step:273 loss:0.05784231424331665\n",
            "iteration:0 step:274 loss:0.04633830487728119\n",
            "iteration:0 step:275 loss:0.07692568004131317\n",
            "iteration:0 step:276 loss:0.05118937790393829\n",
            "iteration:0 step:277 loss:0.10967172682285309\n",
            "iteration:0 step:278 loss:0.08091721683740616\n",
            "iteration:0 step:279 loss:0.017903931438922882\n",
            "iteration:0 step:280 loss:0.04857354983687401\n",
            "iteration:0 step:281 loss:0.0696997418999672\n",
            "iteration:0 step:282 loss:0.1163458377122879\n",
            "iteration:0 step:283 loss:0.026050414890050888\n",
            "iteration:0 step:284 loss:0.08844392001628876\n",
            "iteration:0 step:285 loss:0.020441638305783272\n",
            "iteration:0 step:286 loss:0.0318511426448822\n",
            "iteration:0 step:287 loss:0.05939318984746933\n",
            "iteration:0 step:288 loss:0.04582654684782028\n",
            "iteration:0 step:289 loss:0.12314246594905853\n",
            "iteration:0 step:290 loss:0.03942627087235451\n",
            "iteration:0 step:291 loss:0.0845971405506134\n",
            "iteration:0 step:292 loss:0.1678524613380432\n",
            "iteration:0 step:293 loss:0.03211997449398041\n",
            "iteration:0 step:294 loss:0.05655090510845184\n",
            "iteration:0 step:295 loss:0.12726153433322906\n",
            "iteration:0 step:296 loss:0.028857657685875893\n",
            "iteration:0 step:297 loss:0.023891828954219818\n",
            "iteration:0 step:298 loss:0.0319100059568882\n",
            "iteration:0 step:299 loss:0.08862313628196716\n",
            "iteration:0 step:300 loss:0.04968146234750748\n",
            "iteration:0 step:301 loss:0.07428943365812302\n",
            "iteration:0 step:302 loss:0.051525410264730453\n",
            "iteration:0 step:303 loss:0.011281388811767101\n",
            "iteration:0 step:304 loss:0.06526429206132889\n",
            "iteration:0 step:305 loss:0.018634693697094917\n",
            "iteration:0 step:306 loss:0.06997418403625488\n",
            "iteration:0 step:307 loss:0.1682157963514328\n",
            "iteration:0 step:308 loss:0.041385725140571594\n",
            "iteration:0 step:309 loss:0.05155615881085396\n",
            "iteration:0 step:310 loss:0.049181658774614334\n",
            "iteration:0 step:311 loss:0.057457465678453445\n",
            "iteration:0 step:312 loss:0.05042961984872818\n",
            "iteration:0 step:313 loss:0.054860904812812805\n",
            "iteration:0 step:314 loss:0.05366929620504379\n",
            "iteration:0 step:315 loss:0.0522664375603199\n",
            "iteration:0 step:316 loss:0.02165527269244194\n",
            "iteration:0 step:317 loss:0.046414390206336975\n",
            "iteration:0 step:318 loss:0.07111278176307678\n",
            "iteration:0 step:319 loss:0.03714091330766678\n",
            "iteration:0 step:320 loss:0.09316903352737427\n",
            "iteration:0 step:321 loss:0.06771406531333923\n",
            "iteration:0 step:322 loss:0.11944377422332764\n",
            "iteration:0 step:323 loss:0.09495895355939865\n",
            "iteration:0 step:324 loss:0.09565705060958862\n",
            "iteration:0 step:325 loss:0.03317774832248688\n",
            "iteration:0 step:326 loss:0.047897495329380035\n",
            "iteration:0 step:327 loss:0.08875373005867004\n",
            "iteration:0 step:328 loss:0.03244872763752937\n",
            "iteration:0 step:329 loss:0.04057583585381508\n",
            "iteration:0 step:330 loss:0.06943092495203018\n",
            "iteration:0 step:331 loss:0.0792643129825592\n",
            "iteration:0 step:332 loss:0.1239158883690834\n",
            "iteration:0 step:333 loss:0.03285234421491623\n",
            "iteration:0 step:334 loss:0.08137726038694382\n",
            "iteration:0 step:335 loss:0.07873169332742691\n",
            "iteration:0 step:336 loss:0.10718844085931778\n",
            "iteration:0 step:337 loss:0.028777234256267548\n",
            "iteration:0 step:338 loss:0.015838947147130966\n",
            "iteration:0 step:339 loss:0.04558723792433739\n",
            "iteration:0 step:340 loss:0.05699307471513748\n",
            "iteration:0 step:341 loss:0.06652948260307312\n",
            "iteration:0 step:342 loss:0.06432101875543594\n",
            "iteration:0 step:343 loss:0.056124068796634674\n",
            "iteration:0 step:344 loss:0.07282558083534241\n",
            "iteration:0 step:345 loss:0.02949604205787182\n",
            "iteration:0 step:346 loss:0.06945368647575378\n",
            "iteration:0 step:347 loss:0.1234920471906662\n",
            "iteration:0 step:348 loss:0.05065649747848511\n",
            "iteration:0 step:349 loss:0.026187876239418983\n",
            "iteration:0 step:350 loss:0.07417559623718262\n",
            "iteration:0 step:351 loss:0.060079969465732574\n",
            "iteration:0 step:352 loss:0.057498663663864136\n",
            "iteration:0 step:353 loss:0.056207891553640366\n",
            "iteration:0 step:354 loss:0.04867585375905037\n",
            "iteration:0 step:355 loss:0.0827646404504776\n",
            "iteration:0 step:356 loss:0.10332687199115753\n",
            "iteration:0 step:357 loss:0.049027495086193085\n",
            "iteration:0 step:358 loss:0.0812227800488472\n",
            "iteration:0 step:359 loss:0.08703671395778656\n",
            "iteration:0 step:360 loss:0.08055421710014343\n",
            "iteration:0 step:361 loss:0.15832222998142242\n",
            "iteration:0 step:362 loss:0.06762607395648956\n",
            "iteration:0 step:363 loss:0.02323520928621292\n",
            "iteration:0 step:364 loss:0.02169872634112835\n",
            "iteration:0 step:365 loss:0.09889700263738632\n",
            "iteration:0 step:366 loss:0.04081010818481445\n",
            "iteration:0 step:367 loss:0.0962052196264267\n",
            "iteration:0 step:368 loss:0.049093008041381836\n",
            "iteration:0 step:369 loss:0.12175392359495163\n",
            "iteration:0 step:370 loss:0.09549888223409653\n",
            "iteration:0 step:371 loss:0.12449275702238083\n",
            "iteration:0 step:372 loss:0.07417302578687668\n",
            "iteration:0 step:373 loss:0.04874685779213905\n",
            "iteration:0 step:374 loss:0.10325192660093307\n",
            "iteration:0 step:375 loss:0.05395137146115303\n",
            "iteration:0 step:376 loss:0.017270173877477646\n",
            "iteration:0 step:377 loss:0.049267131835222244\n",
            "iteration:0 step:378 loss:0.028892742469906807\n",
            "iteration:0 step:379 loss:0.051320482045412064\n",
            "iteration:0 step:380 loss:0.034425899386405945\n",
            "iteration:0 step:381 loss:0.03766201436519623\n",
            "iteration:0 step:382 loss:0.20770637691020966\n",
            "iteration:0 step:383 loss:0.048760294914245605\n",
            "iteration:0 step:384 loss:0.0439302921295166\n",
            "iteration:0 step:385 loss:0.025560574606060982\n",
            "iteration:0 step:386 loss:0.09305012226104736\n",
            "iteration:0 step:387 loss:0.12571397423744202\n",
            "iteration:0 step:388 loss:0.04235680773854256\n",
            "iteration:0 step:389 loss:0.12894900143146515\n",
            "iteration:0 step:390 loss:0.033915188163518906\n",
            "iteration:0 step:391 loss:0.04001179337501526\n",
            "iteration:0 step:392 loss:0.09395349770784378\n",
            "iteration:0 step:393 loss:0.13607344031333923\n",
            "iteration:0 step:394 loss:0.08791203051805496\n",
            "iteration:0 step:395 loss:0.02221638523042202\n",
            "iteration:0 step:396 loss:0.048328276723623276\n",
            "iteration:0 step:397 loss:0.053395070135593414\n",
            "iteration:0 step:398 loss:0.01944429986178875\n",
            "iteration:0 step:399 loss:0.03915208578109741\n",
            "iteration:0 step:400 loss:0.09037496149539948\n",
            "iteration:0 step:401 loss:0.03454302251338959\n",
            "iteration:0 step:402 loss:0.04677479341626167\n",
            "iteration:0 step:403 loss:0.061465658247470856\n",
            "iteration:0 step:404 loss:0.043453194200992584\n",
            "iteration:0 step:405 loss:0.02762725204229355\n",
            "iteration:0 step:406 loss:0.06461548060178757\n",
            "iteration:0 step:407 loss:0.0764583870768547\n",
            "iteration:0 step:408 loss:0.06734124571084976\n",
            "iteration:0 step:409 loss:0.042137376964092255\n",
            "iteration:0 step:410 loss:0.021691812202334404\n",
            "iteration:0 step:411 loss:0.042087748646736145\n",
            "iteration:0 step:412 loss:0.077886201441288\n",
            "iteration:0 step:413 loss:0.127284973859787\n",
            "iteration:0 step:414 loss:0.056702133268117905\n",
            "iteration:0 step:415 loss:0.058355506509542465\n",
            "iteration:0 step:416 loss:0.0099228760227561\n",
            "iteration:0 step:417 loss:0.05354690924286842\n",
            "iteration:0 step:418 loss:0.056501321494579315\n",
            "iteration:0 step:419 loss:0.08090271055698395\n",
            "iteration:0 step:420 loss:0.027613848447799683\n",
            "iteration:0 step:421 loss:0.04242223501205444\n",
            "iteration:0 step:422 loss:0.047712646424770355\n",
            "iteration:0 step:423 loss:0.06011899560689926\n",
            "iteration:0 step:424 loss:0.03111857734620571\n",
            "iteration:0 step:425 loss:0.047792721539735794\n",
            "iteration:0 step:426 loss:0.05748654156923294\n",
            "iteration:0 step:427 loss:0.014187956228852272\n",
            "iteration:0 step:428 loss:0.07275141030550003\n",
            "iteration:0 step:429 loss:0.05084555223584175\n",
            "iteration:0 step:430 loss:0.01739770546555519\n",
            "iteration:0 step:431 loss:0.05745314806699753\n",
            "iteration:0 step:432 loss:0.07325699925422668\n",
            "iteration:0 step:433 loss:0.06616942584514618\n",
            "iteration:0 step:434 loss:0.034654468297958374\n",
            "iteration:0 step:435 loss:0.07400435954332352\n",
            "iteration:0 step:436 loss:0.05621242895722389\n",
            "iteration:0 step:437 loss:0.043747249990701675\n",
            "iteration:0 step:438 loss:0.041908808052539825\n",
            "iteration:0 step:439 loss:0.053235359489917755\n",
            "iteration:0 step:440 loss:0.04175955802202225\n",
            "iteration:0 step:441 loss:0.05664702132344246\n",
            "iteration:0 step:442 loss:0.06301054358482361\n",
            "iteration:0 step:443 loss:0.045440372079610825\n",
            "iteration:0 step:444 loss:0.03464220464229584\n",
            "iteration:0 step:445 loss:0.059135012328624725\n",
            "iteration:0 step:446 loss:0.016824422404170036\n",
            "iteration:0 step:447 loss:0.050336312502622604\n",
            "iteration:0 step:448 loss:0.03502641245722771\n",
            "iteration:0 step:449 loss:0.0321204699575901\n",
            "iteration:0 step:450 loss:0.04998175799846649\n",
            "iteration:0 step:451 loss:0.04687058925628662\n",
            "iteration:0 step:452 loss:0.023317579180002213\n",
            "iteration:0 step:453 loss:0.06049896031618118\n",
            "iteration:0 step:454 loss:0.014471465721726418\n",
            "iteration:0 step:455 loss:0.016461782157421112\n",
            "iteration:0 step:456 loss:0.02057255432009697\n",
            "iteration:0 step:457 loss:0.009570138528943062\n",
            "iteration:0 step:458 loss:0.02184280753135681\n",
            "iteration:0 step:459 loss:0.07193835824728012\n",
            "iteration:0 step:460 loss:0.004239351954311132\n",
            "iteration:0 step:461 loss:0.005171047989279032\n",
            "iteration:0 step:462 loss:0.00515683414414525\n",
            "iteration:0 step:463 loss:0.0853862315416336\n",
            "iteration:0 step:464 loss:0.031115246936678886\n",
            "iteration:0 step:465 loss:0.008958928287029266\n",
            "iteration:0 step:466 loss:0.21278084814548492\n",
            "iteration:0 step:467 loss:0.006695065181702375\n",
            "iteration:0 step:468 loss:0.1586109846830368\n",
            "iteration:1 step:0 loss:0.0363595187664032\n",
            "iteration:1 step:1 loss:0.11509571969509125\n",
            "iteration:1 step:2 loss:0.022478895261883736\n",
            "iteration:1 step:3 loss:0.1289646178483963\n",
            "iteration:1 step:4 loss:0.04947217181324959\n",
            "iteration:1 step:5 loss:0.07784750312566757\n",
            "iteration:1 step:6 loss:0.05493990331888199\n",
            "iteration:1 step:7 loss:0.04265699163079262\n",
            "iteration:1 step:8 loss:0.12988245487213135\n",
            "iteration:1 step:9 loss:0.10093936324119568\n",
            "iteration:1 step:10 loss:0.0988033190369606\n",
            "iteration:1 step:11 loss:0.02537207305431366\n",
            "iteration:1 step:12 loss:0.06484909355640411\n",
            "iteration:1 step:13 loss:0.0353960283100605\n",
            "iteration:1 step:14 loss:0.019850263372063637\n",
            "iteration:1 step:15 loss:0.06273507326841354\n",
            "iteration:1 step:16 loss:0.019512807950377464\n",
            "iteration:1 step:17 loss:0.023653525859117508\n",
            "iteration:1 step:18 loss:0.02807513438165188\n",
            "iteration:1 step:19 loss:0.016490094363689423\n",
            "iteration:1 step:20 loss:0.07692694664001465\n",
            "iteration:1 step:21 loss:0.06685291230678558\n",
            "iteration:1 step:22 loss:0.02412726730108261\n",
            "iteration:1 step:23 loss:0.041829150170087814\n",
            "iteration:1 step:24 loss:0.013637066818773746\n",
            "iteration:1 step:25 loss:0.02633366361260414\n",
            "iteration:1 step:26 loss:0.024014214053750038\n",
            "iteration:1 step:27 loss:0.03989779204130173\n",
            "iteration:1 step:28 loss:0.053309116512537\n",
            "iteration:1 step:29 loss:0.036298129707574844\n",
            "iteration:1 step:30 loss:0.014659365639090538\n",
            "iteration:1 step:31 loss:0.04139311611652374\n",
            "iteration:1 step:32 loss:0.031239895150065422\n",
            "iteration:1 step:33 loss:0.03504757583141327\n",
            "iteration:1 step:34 loss:0.06064111366868019\n",
            "iteration:1 step:35 loss:0.014325186610221863\n",
            "iteration:1 step:36 loss:0.07970631867647171\n",
            "iteration:1 step:37 loss:0.020544251427054405\n",
            "iteration:1 step:38 loss:0.037623174488544464\n",
            "iteration:1 step:39 loss:0.06744659692049026\n",
            "iteration:1 step:40 loss:0.07064948230981827\n",
            "iteration:1 step:41 loss:0.05475344508886337\n",
            "iteration:1 step:42 loss:0.013257748447358608\n",
            "iteration:1 step:43 loss:0.038910314440727234\n",
            "iteration:1 step:44 loss:0.06250471621751785\n",
            "iteration:1 step:45 loss:0.07075577974319458\n",
            "iteration:1 step:46 loss:0.051664408296346664\n",
            "iteration:1 step:47 loss:0.032046034932136536\n",
            "iteration:1 step:48 loss:0.04300449788570404\n",
            "iteration:1 step:49 loss:0.015594852156937122\n",
            "iteration:1 step:50 loss:0.021043600514531136\n",
            "iteration:1 step:51 loss:0.02640555053949356\n",
            "iteration:1 step:52 loss:0.0268551018089056\n",
            "iteration:1 step:53 loss:0.14320838451385498\n",
            "iteration:1 step:54 loss:0.08318523317575455\n",
            "iteration:1 step:55 loss:0.06813649088144302\n",
            "iteration:1 step:56 loss:0.08631499856710434\n",
            "iteration:1 step:57 loss:0.05859322473406792\n",
            "iteration:1 step:58 loss:0.037294309586286545\n",
            "iteration:1 step:59 loss:0.061616286635398865\n",
            "iteration:1 step:60 loss:0.043150972574949265\n",
            "iteration:1 step:61 loss:0.05233736336231232\n",
            "iteration:1 step:62 loss:0.048497430980205536\n",
            "iteration:1 step:63 loss:0.06293262541294098\n",
            "iteration:1 step:64 loss:0.15101853013038635\n",
            "iteration:1 step:65 loss:0.020831840112805367\n",
            "iteration:1 step:66 loss:0.05952812731266022\n",
            "iteration:1 step:67 loss:0.04022855684161186\n",
            "iteration:1 step:68 loss:0.11339419335126877\n",
            "iteration:1 step:69 loss:0.10677756369113922\n",
            "iteration:1 step:70 loss:0.05235016345977783\n",
            "iteration:1 step:71 loss:0.03665544092655182\n",
            "iteration:1 step:72 loss:0.09068799018859863\n",
            "iteration:1 step:73 loss:0.055706094950437546\n",
            "iteration:1 step:74 loss:0.03259680047631264\n",
            "iteration:1 step:75 loss:0.030498428270220757\n",
            "iteration:1 step:76 loss:0.03794720396399498\n",
            "iteration:1 step:77 loss:0.0262594111263752\n",
            "iteration:1 step:78 loss:0.03934066742658615\n",
            "iteration:1 step:79 loss:0.08451397716999054\n",
            "iteration:1 step:80 loss:0.09135626256465912\n",
            "iteration:1 step:81 loss:0.022455500438809395\n",
            "iteration:1 step:82 loss:0.007328820414841175\n",
            "iteration:1 step:83 loss:0.025286449119448662\n",
            "iteration:1 step:84 loss:0.037658654153347015\n",
            "iteration:1 step:85 loss:0.0602760873734951\n",
            "iteration:1 step:86 loss:0.022156840190291405\n",
            "iteration:1 step:87 loss:0.05811942741274834\n",
            "iteration:1 step:88 loss:0.025210503488779068\n",
            "iteration:1 step:89 loss:0.019147317856550217\n",
            "iteration:1 step:90 loss:0.09433399140834808\n",
            "iteration:1 step:91 loss:0.08856485784053802\n",
            "iteration:1 step:92 loss:0.06427004933357239\n",
            "iteration:1 step:93 loss:0.07919342815876007\n",
            "iteration:1 step:94 loss:0.011826187372207642\n",
            "iteration:1 step:95 loss:0.03721632808446884\n",
            "iteration:1 step:96 loss:0.034928373992443085\n",
            "iteration:1 step:97 loss:0.036353692412376404\n",
            "iteration:1 step:98 loss:0.12450921535491943\n",
            "iteration:1 step:99 loss:0.07638879865407944\n",
            "iteration:1 step:100 loss:0.031381379812955856\n",
            "iteration:1 step:101 loss:0.04301487281918526\n",
            "iteration:1 step:102 loss:0.026188991963863373\n",
            "iteration:1 step:103 loss:0.030669724568724632\n",
            "iteration:1 step:104 loss:0.03812842816114426\n",
            "iteration:1 step:105 loss:0.037157878279685974\n",
            "iteration:1 step:106 loss:0.055655404925346375\n",
            "iteration:1 step:107 loss:0.03930887207388878\n",
            "iteration:1 step:108 loss:0.038500647991895676\n",
            "iteration:1 step:109 loss:0.07587587088346481\n",
            "iteration:1 step:110 loss:0.03643155097961426\n",
            "iteration:1 step:111 loss:0.054836444556713104\n",
            "iteration:1 step:112 loss:0.04723859578371048\n",
            "iteration:1 step:113 loss:0.05723653361201286\n",
            "iteration:1 step:114 loss:0.03530815243721008\n",
            "iteration:1 step:115 loss:0.07340288162231445\n",
            "iteration:1 step:116 loss:0.017461461946368217\n",
            "iteration:1 step:117 loss:0.027911875396966934\n",
            "iteration:1 step:118 loss:0.0461614653468132\n",
            "iteration:1 step:119 loss:0.0668078362941742\n",
            "iteration:1 step:120 loss:0.03961716219782829\n",
            "iteration:1 step:121 loss:0.03750200197100639\n",
            "iteration:1 step:122 loss:0.03084825910627842\n",
            "iteration:1 step:123 loss:0.11941780894994736\n",
            "iteration:1 step:124 loss:0.06706012040376663\n",
            "iteration:1 step:125 loss:0.04975467175245285\n",
            "iteration:1 step:126 loss:0.03752413019537926\n",
            "iteration:1 step:127 loss:0.05439504608511925\n",
            "iteration:1 step:128 loss:0.04747236520051956\n",
            "iteration:1 step:129 loss:0.037737034261226654\n",
            "iteration:1 step:130 loss:0.07675395905971527\n",
            "iteration:1 step:131 loss:0.02907242253422737\n",
            "iteration:1 step:132 loss:0.05834338441491127\n",
            "iteration:1 step:133 loss:0.07825781404972076\n",
            "iteration:1 step:134 loss:0.06802018731832504\n",
            "iteration:1 step:135 loss:0.02732125110924244\n",
            "iteration:1 step:136 loss:0.03387080505490303\n",
            "iteration:1 step:137 loss:0.03921085596084595\n",
            "iteration:1 step:138 loss:0.10414617508649826\n",
            "iteration:1 step:139 loss:0.06562669575214386\n",
            "iteration:1 step:140 loss:0.041214391589164734\n",
            "iteration:1 step:141 loss:0.03869577497243881\n",
            "iteration:1 step:142 loss:0.025509299710392952\n",
            "iteration:1 step:143 loss:0.044685982167720795\n",
            "iteration:1 step:144 loss:0.04036133736371994\n",
            "iteration:1 step:145 loss:0.03606637194752693\n",
            "iteration:1 step:146 loss:0.02878653071820736\n",
            "iteration:1 step:147 loss:0.01171134877949953\n",
            "iteration:1 step:148 loss:0.015229413285851479\n",
            "iteration:1 step:149 loss:0.07452130317687988\n",
            "iteration:1 step:150 loss:0.021645735949277878\n",
            "iteration:1 step:151 loss:0.09133567661046982\n",
            "iteration:1 step:152 loss:0.0754057988524437\n",
            "iteration:1 step:153 loss:0.020498836413025856\n",
            "iteration:1 step:154 loss:0.034047599881887436\n",
            "iteration:1 step:155 loss:0.04745936021208763\n",
            "iteration:1 step:156 loss:0.05746054649353027\n",
            "iteration:1 step:157 loss:0.054540764540433884\n",
            "iteration:1 step:158 loss:0.062016233801841736\n",
            "iteration:1 step:159 loss:0.009303863160312176\n",
            "iteration:1 step:160 loss:0.027880031615495682\n",
            "iteration:1 step:161 loss:0.11489876359701157\n",
            "iteration:1 step:162 loss:0.1021171510219574\n",
            "iteration:1 step:163 loss:0.0586828850209713\n",
            "iteration:1 step:164 loss:0.037178944796323776\n",
            "iteration:1 step:165 loss:0.03517504036426544\n",
            "iteration:1 step:166 loss:0.023036543279886246\n",
            "iteration:1 step:167 loss:0.029145430773496628\n",
            "iteration:1 step:168 loss:0.09972982853651047\n",
            "iteration:1 step:169 loss:0.023340575397014618\n",
            "iteration:1 step:170 loss:0.010058696381747723\n",
            "iteration:1 step:171 loss:0.06354779005050659\n",
            "iteration:1 step:172 loss:0.027255970984697342\n",
            "iteration:1 step:173 loss:0.09064605087041855\n",
            "iteration:1 step:174 loss:0.030240409076213837\n",
            "iteration:1 step:175 loss:0.06568676978349686\n",
            "iteration:1 step:176 loss:0.08410824090242386\n",
            "iteration:1 step:177 loss:0.02677110955119133\n",
            "iteration:1 step:178 loss:0.02161523886024952\n",
            "iteration:1 step:179 loss:0.026500973850488663\n",
            "iteration:1 step:180 loss:0.03243877738714218\n",
            "iteration:1 step:181 loss:0.04943107068538666\n",
            "iteration:1 step:182 loss:0.05446920543909073\n",
            "iteration:1 step:183 loss:0.02172066643834114\n",
            "iteration:1 step:184 loss:0.0758400410413742\n",
            "iteration:1 step:185 loss:0.046310536563396454\n",
            "iteration:1 step:186 loss:0.07650817930698395\n",
            "iteration:1 step:187 loss:0.07011497765779495\n",
            "iteration:1 step:188 loss:0.020466884598135948\n",
            "iteration:1 step:189 loss:0.05857567861676216\n",
            "iteration:1 step:190 loss:0.005572705529630184\n",
            "iteration:1 step:191 loss:0.047469932585954666\n",
            "iteration:1 step:192 loss:0.02946958690881729\n",
            "iteration:1 step:193 loss:0.05859912186861038\n",
            "iteration:1 step:194 loss:0.02742357738316059\n",
            "iteration:1 step:195 loss:0.009303117170929909\n",
            "iteration:1 step:196 loss:0.07079256325960159\n",
            "iteration:1 step:197 loss:0.05054387450218201\n",
            "iteration:1 step:198 loss:0.0065468354150652885\n",
            "iteration:1 step:199 loss:0.10295245051383972\n",
            "iteration:1 step:200 loss:0.0388692244887352\n",
            "iteration:1 step:201 loss:0.05674208700656891\n",
            "iteration:1 step:202 loss:0.02837369590997696\n",
            "iteration:1 step:203 loss:0.031561847776174545\n",
            "iteration:1 step:204 loss:0.019620949402451515\n",
            "iteration:1 step:205 loss:0.050323132425546646\n",
            "iteration:1 step:206 loss:0.0795033648610115\n",
            "iteration:1 step:207 loss:0.1345466524362564\n",
            "iteration:1 step:208 loss:0.13617849349975586\n",
            "iteration:1 step:209 loss:0.04060833528637886\n",
            "iteration:1 step:210 loss:0.06400217115879059\n",
            "iteration:1 step:211 loss:0.01582491584122181\n",
            "iteration:1 step:212 loss:0.15483713150024414\n",
            "iteration:1 step:213 loss:0.012015530839562416\n",
            "iteration:1 step:214 loss:0.054152753204107285\n",
            "iteration:1 step:215 loss:0.045503269881010056\n",
            "iteration:1 step:216 loss:0.05029209703207016\n",
            "iteration:1 step:217 loss:0.10630296170711517\n",
            "iteration:1 step:218 loss:0.02114250510931015\n",
            "iteration:1 step:219 loss:0.03399326652288437\n",
            "iteration:1 step:220 loss:0.040350060909986496\n",
            "iteration:1 step:221 loss:0.07816063612699509\n",
            "iteration:1 step:222 loss:0.019581161439418793\n",
            "iteration:1 step:223 loss:0.05153268203139305\n",
            "iteration:1 step:224 loss:0.0520557165145874\n",
            "iteration:1 step:225 loss:0.022428326308727264\n",
            "iteration:1 step:226 loss:0.06935568898916245\n",
            "iteration:1 step:227 loss:0.043104179203510284\n",
            "iteration:1 step:228 loss:0.06081455200910568\n",
            "iteration:1 step:229 loss:0.05245625972747803\n",
            "iteration:1 step:230 loss:0.029751332476735115\n",
            "iteration:1 step:231 loss:0.03374961391091347\n",
            "iteration:1 step:232 loss:0.0346151739358902\n",
            "iteration:1 step:233 loss:0.048864394426345825\n",
            "iteration:1 step:234 loss:0.05585207790136337\n",
            "iteration:1 step:235 loss:0.04807755723595619\n",
            "iteration:1 step:236 loss:0.0208674855530262\n",
            "iteration:1 step:237 loss:0.020550891757011414\n",
            "iteration:1 step:238 loss:0.04751764237880707\n",
            "iteration:1 step:239 loss:0.058481112122535706\n",
            "iteration:1 step:240 loss:0.03417312353849411\n",
            "iteration:1 step:241 loss:0.04898485168814659\n",
            "iteration:1 step:242 loss:0.028303710743784904\n",
            "iteration:1 step:243 loss:0.11348952353000641\n",
            "iteration:1 step:244 loss:0.038869477808475494\n",
            "iteration:1 step:245 loss:0.037943318486213684\n",
            "iteration:1 step:246 loss:0.06454968452453613\n",
            "iteration:1 step:247 loss:0.06816912442445755\n",
            "iteration:1 step:248 loss:0.05678349733352661\n",
            "iteration:1 step:249 loss:0.037933796644210815\n",
            "iteration:1 step:250 loss:0.06323982775211334\n",
            "iteration:1 step:251 loss:0.04542958736419678\n",
            "iteration:1 step:252 loss:0.05255463719367981\n",
            "iteration:1 step:253 loss:0.07918648421764374\n",
            "iteration:1 step:254 loss:0.030025972053408623\n",
            "iteration:1 step:255 loss:0.06082843244075775\n",
            "iteration:1 step:256 loss:0.03745722770690918\n",
            "iteration:1 step:257 loss:0.013383142650127411\n",
            "iteration:1 step:258 loss:0.02893424779176712\n",
            "iteration:1 step:259 loss:0.02672983705997467\n",
            "iteration:1 step:260 loss:0.07908258587121964\n",
            "iteration:1 step:261 loss:0.07049547880887985\n",
            "iteration:1 step:262 loss:0.02121872641146183\n",
            "iteration:1 step:263 loss:0.0533212348818779\n",
            "iteration:1 step:264 loss:0.009574322029948235\n",
            "iteration:1 step:265 loss:0.01908489502966404\n",
            "iteration:1 step:266 loss:0.047822996973991394\n",
            "iteration:1 step:267 loss:0.012179562821984291\n",
            "iteration:1 step:268 loss:0.0905291959643364\n",
            "iteration:1 step:269 loss:0.0474032461643219\n",
            "iteration:1 step:270 loss:0.061783041805028915\n",
            "iteration:1 step:271 loss:0.10005579143762589\n",
            "iteration:1 step:272 loss:0.06437092274427414\n",
            "iteration:1 step:273 loss:0.045269675552845\n",
            "iteration:1 step:274 loss:0.03404738008975983\n",
            "iteration:1 step:275 loss:0.06332474201917648\n",
            "iteration:1 step:276 loss:0.0403444766998291\n",
            "iteration:1 step:277 loss:0.08977596461772919\n",
            "iteration:1 step:278 loss:0.07197770476341248\n",
            "iteration:1 step:279 loss:0.01354766171425581\n",
            "iteration:1 step:280 loss:0.03767426684498787\n",
            "iteration:1 step:281 loss:0.05953318998217583\n",
            "iteration:1 step:282 loss:0.10154657065868378\n",
            "iteration:1 step:283 loss:0.020488983020186424\n",
            "iteration:1 step:284 loss:0.07239627093076706\n",
            "iteration:1 step:285 loss:0.010870006866753101\n",
            "iteration:1 step:286 loss:0.01766493357717991\n",
            "iteration:1 step:287 loss:0.04605890437960625\n",
            "iteration:1 step:288 loss:0.03412103280425072\n",
            "iteration:1 step:289 loss:0.0984199121594429\n",
            "iteration:1 step:290 loss:0.025559773668646812\n",
            "iteration:1 step:291 loss:0.047850389033555984\n",
            "iteration:1 step:292 loss:0.11591801047325134\n",
            "iteration:1 step:293 loss:0.022580208256840706\n",
            "iteration:1 step:294 loss:0.038237955421209335\n",
            "iteration:1 step:295 loss:0.10886018723249435\n",
            "iteration:1 step:296 loss:0.018007665872573853\n",
            "iteration:1 step:297 loss:0.017876770347356796\n",
            "iteration:1 step:298 loss:0.031817756593227386\n",
            "iteration:1 step:299 loss:0.07720569521188736\n",
            "iteration:1 step:300 loss:0.03467937186360359\n",
            "iteration:1 step:301 loss:0.05635478347539902\n",
            "iteration:1 step:302 loss:0.041017282754182816\n",
            "iteration:1 step:303 loss:0.00781605951488018\n",
            "iteration:1 step:304 loss:0.05640685185790062\n",
            "iteration:1 step:305 loss:0.013073474168777466\n",
            "iteration:1 step:306 loss:0.050393979996442795\n",
            "iteration:1 step:307 loss:0.13950426876544952\n",
            "iteration:1 step:308 loss:0.031207047402858734\n",
            "iteration:1 step:309 loss:0.03320103883743286\n",
            "iteration:1 step:310 loss:0.04181087389588356\n",
            "iteration:1 step:311 loss:0.03833438828587532\n",
            "iteration:1 step:312 loss:0.03710996359586716\n",
            "iteration:1 step:313 loss:0.04410669580101967\n",
            "iteration:1 step:314 loss:0.04337996616959572\n",
            "iteration:1 step:315 loss:0.03259483724832535\n",
            "iteration:1 step:316 loss:0.012547027319669724\n",
            "iteration:1 step:317 loss:0.030218549072742462\n",
            "iteration:1 step:318 loss:0.0593879334628582\n",
            "iteration:1 step:319 loss:0.033464327454566956\n",
            "iteration:1 step:320 loss:0.06655265390872955\n",
            "iteration:1 step:321 loss:0.05945742130279541\n",
            "iteration:1 step:322 loss:0.07999067008495331\n",
            "iteration:1 step:323 loss:0.05848058685660362\n",
            "iteration:1 step:324 loss:0.07816682010889053\n",
            "iteration:1 step:325 loss:0.022769207134842873\n",
            "iteration:1 step:326 loss:0.02974891848862171\n",
            "iteration:1 step:327 loss:0.06770340353250504\n",
            "iteration:1 step:328 loss:0.021408509463071823\n",
            "iteration:1 step:329 loss:0.030434731394052505\n",
            "iteration:1 step:330 loss:0.05333734303712845\n",
            "iteration:1 step:331 loss:0.05367918685078621\n",
            "iteration:1 step:332 loss:0.10007786750793457\n",
            "iteration:1 step:333 loss:0.026851078495383263\n",
            "iteration:1 step:334 loss:0.06457708775997162\n",
            "iteration:1 step:335 loss:0.06251442432403564\n",
            "iteration:1 step:336 loss:0.09371648728847504\n",
            "iteration:1 step:337 loss:0.021695967763662338\n",
            "iteration:1 step:338 loss:0.013443035073578358\n",
            "iteration:1 step:339 loss:0.03731674328446388\n",
            "iteration:1 step:340 loss:0.04620566964149475\n",
            "iteration:1 step:341 loss:0.055364735424518585\n",
            "iteration:1 step:342 loss:0.05631023645401001\n",
            "iteration:1 step:343 loss:0.04036872461438179\n",
            "iteration:1 step:344 loss:0.061778489500284195\n",
            "iteration:1 step:345 loss:0.017290180549025536\n",
            "iteration:1 step:346 loss:0.05312807485461235\n",
            "iteration:1 step:347 loss:0.09888516366481781\n",
            "iteration:1 step:348 loss:0.036135878413915634\n",
            "iteration:1 step:349 loss:0.018442148342728615\n",
            "iteration:1 step:350 loss:0.0459337942302227\n",
            "iteration:1 step:351 loss:0.03711762651801109\n",
            "iteration:1 step:352 loss:0.04936496913433075\n",
            "iteration:1 step:353 loss:0.04176890105009079\n",
            "iteration:1 step:354 loss:0.03762618452310562\n",
            "iteration:1 step:355 loss:0.0635690838098526\n",
            "iteration:1 step:356 loss:0.06669842451810837\n",
            "iteration:1 step:357 loss:0.025354918092489243\n",
            "iteration:1 step:358 loss:0.06592749804258347\n",
            "iteration:1 step:359 loss:0.06513828039169312\n",
            "iteration:1 step:360 loss:0.05246833339333534\n",
            "iteration:1 step:361 loss:0.12531441450119019\n",
            "iteration:1 step:362 loss:0.04090866446495056\n",
            "iteration:1 step:363 loss:0.016098443418741226\n",
            "iteration:1 step:364 loss:0.014019832946360111\n",
            "iteration:1 step:365 loss:0.08714589476585388\n",
            "iteration:1 step:366 loss:0.03394359350204468\n",
            "iteration:1 step:367 loss:0.08117596805095673\n",
            "iteration:1 step:368 loss:0.033702317625284195\n",
            "iteration:1 step:369 loss:0.09633732587099075\n",
            "iteration:1 step:370 loss:0.07307598739862442\n",
            "iteration:1 step:371 loss:0.11937517672777176\n",
            "iteration:1 step:372 loss:0.0592641681432724\n",
            "iteration:1 step:373 loss:0.030654773116111755\n",
            "iteration:1 step:374 loss:0.08058890700340271\n",
            "iteration:1 step:375 loss:0.042458534240722656\n",
            "iteration:1 step:376 loss:0.013822786509990692\n",
            "iteration:1 step:377 loss:0.03322932869195938\n",
            "iteration:1 step:378 loss:0.020050911232829094\n",
            "iteration:1 step:379 loss:0.03758137300610542\n",
            "iteration:1 step:380 loss:0.02169819362461567\n",
            "iteration:1 step:381 loss:0.03295852616429329\n",
            "iteration:1 step:382 loss:0.1601833701133728\n",
            "iteration:1 step:383 loss:0.037775877863168716\n",
            "iteration:1 step:384 loss:0.033983178436756134\n",
            "iteration:1 step:385 loss:0.02036459743976593\n",
            "iteration:1 step:386 loss:0.0693383514881134\n",
            "iteration:1 step:387 loss:0.08927302062511444\n",
            "iteration:1 step:388 loss:0.029443789273500443\n",
            "iteration:1 step:389 loss:0.09729201346635818\n",
            "iteration:1 step:390 loss:0.027483254671096802\n",
            "iteration:1 step:391 loss:0.022963276132941246\n",
            "iteration:1 step:392 loss:0.06955435127019882\n",
            "iteration:1 step:393 loss:0.09530274569988251\n",
            "iteration:1 step:394 loss:0.06416065990924835\n",
            "iteration:1 step:395 loss:0.020195037126541138\n",
            "iteration:1 step:396 loss:0.03157954663038254\n",
            "iteration:1 step:397 loss:0.037976402789354324\n",
            "iteration:1 step:398 loss:0.016042359173297882\n",
            "iteration:1 step:399 loss:0.02956520952284336\n",
            "iteration:1 step:400 loss:0.07562513649463654\n",
            "iteration:1 step:401 loss:0.030276605859398842\n",
            "iteration:1 step:402 loss:0.03338904306292534\n",
            "iteration:1 step:403 loss:0.04355546459555626\n",
            "iteration:1 step:404 loss:0.029475020244717598\n",
            "iteration:1 step:405 loss:0.02499115653336048\n",
            "iteration:1 step:406 loss:0.04930837079882622\n",
            "iteration:1 step:407 loss:0.05437234789133072\n",
            "iteration:1 step:408 loss:0.0496055968105793\n",
            "iteration:1 step:409 loss:0.02996859699487686\n",
            "iteration:1 step:410 loss:0.016829954460263252\n",
            "iteration:1 step:411 loss:0.027969690039753914\n",
            "iteration:1 step:412 loss:0.06366690248250961\n",
            "iteration:1 step:413 loss:0.08453255891799927\n",
            "iteration:1 step:414 loss:0.04400021955370903\n",
            "iteration:1 step:415 loss:0.04165062680840492\n",
            "iteration:1 step:416 loss:0.0068649426102638245\n",
            "iteration:1 step:417 loss:0.04106364771723747\n",
            "iteration:1 step:418 loss:0.034638985991477966\n",
            "iteration:1 step:419 loss:0.05892898142337799\n",
            "iteration:1 step:420 loss:0.0148453488945961\n",
            "iteration:1 step:421 loss:0.024671019986271858\n",
            "iteration:1 step:422 loss:0.03884555399417877\n",
            "iteration:1 step:423 loss:0.04378518834710121\n",
            "iteration:1 step:424 loss:0.019705291837453842\n",
            "iteration:1 step:425 loss:0.03382842615246773\n",
            "iteration:1 step:426 loss:0.04629290848970413\n",
            "iteration:1 step:427 loss:0.009169836528599262\n",
            "iteration:1 step:428 loss:0.053050436079502106\n",
            "iteration:1 step:429 loss:0.03369568660855293\n",
            "iteration:1 step:430 loss:0.011082473210990429\n",
            "iteration:1 step:431 loss:0.0477786660194397\n",
            "iteration:1 step:432 loss:0.06626126170158386\n",
            "iteration:1 step:433 loss:0.043145205825567245\n",
            "iteration:1 step:434 loss:0.023557808250188828\n",
            "iteration:1 step:435 loss:0.05223464220762253\n",
            "iteration:1 step:436 loss:0.03771831840276718\n",
            "iteration:1 step:437 loss:0.026526441797614098\n",
            "iteration:1 step:438 loss:0.03155957907438278\n",
            "iteration:1 step:439 loss:0.03814464434981346\n",
            "iteration:1 step:440 loss:0.03201722726225853\n",
            "iteration:1 step:441 loss:0.03757600858807564\n",
            "iteration:1 step:442 loss:0.042280495166778564\n",
            "iteration:1 step:443 loss:0.033430591225624084\n",
            "iteration:1 step:444 loss:0.019113343209028244\n",
            "iteration:1 step:445 loss:0.03578077629208565\n",
            "iteration:1 step:446 loss:0.01236996054649353\n",
            "iteration:1 step:447 loss:0.03146062046289444\n",
            "iteration:1 step:448 loss:0.029663914814591408\n",
            "iteration:1 step:449 loss:0.028975380584597588\n",
            "iteration:1 step:450 loss:0.03262372687458992\n",
            "iteration:1 step:451 loss:0.03886732459068298\n",
            "iteration:1 step:452 loss:0.015806561335921288\n",
            "iteration:1 step:453 loss:0.04956384748220444\n",
            "iteration:1 step:454 loss:0.011089327745139599\n",
            "iteration:1 step:455 loss:0.010098370723426342\n",
            "iteration:1 step:456 loss:0.01443568803369999\n",
            "iteration:1 step:457 loss:0.007882315665483475\n",
            "iteration:1 step:458 loss:0.014945845119655132\n",
            "iteration:1 step:459 loss:0.03802725672721863\n",
            "iteration:1 step:460 loss:0.002878789324313402\n",
            "iteration:1 step:461 loss:0.004130277782678604\n",
            "iteration:1 step:462 loss:0.00426750723272562\n",
            "iteration:1 step:463 loss:0.07246608287096024\n",
            "iteration:1 step:464 loss:0.02369595691561699\n",
            "iteration:1 step:465 loss:0.006593171041458845\n",
            "iteration:1 step:466 loss:0.17110711336135864\n",
            "iteration:1 step:467 loss:0.005535106174647808\n",
            "iteration:1 step:468 loss:0.13278673589229584\n",
            "iteration:2 step:0 loss:0.030087174847722054\n",
            "iteration:2 step:1 loss:0.10195184499025345\n",
            "iteration:2 step:2 loss:0.015166296623647213\n",
            "iteration:2 step:3 loss:0.11543909460306168\n",
            "iteration:2 step:4 loss:0.036560408771038055\n",
            "iteration:2 step:5 loss:0.05756448954343796\n",
            "iteration:2 step:6 loss:0.0421425886452198\n",
            "iteration:2 step:7 loss:0.029958024621009827\n",
            "iteration:2 step:8 loss:0.08647554367780685\n",
            "iteration:2 step:9 loss:0.07397658377885818\n",
            "iteration:2 step:10 loss:0.0903085470199585\n",
            "iteration:2 step:11 loss:0.02055121213197708\n",
            "iteration:2 step:12 loss:0.05559650436043739\n",
            "iteration:2 step:13 loss:0.029056137427687645\n",
            "iteration:2 step:14 loss:0.01595410518348217\n",
            "iteration:2 step:15 loss:0.0480552613735199\n",
            "iteration:2 step:16 loss:0.010642026551067829\n",
            "iteration:2 step:17 loss:0.015053784474730492\n",
            "iteration:2 step:18 loss:0.020344745367765427\n",
            "iteration:2 step:19 loss:0.0093564847484231\n",
            "iteration:2 step:20 loss:0.05969963222742081\n",
            "iteration:2 step:21 loss:0.05209365487098694\n",
            "iteration:2 step:22 loss:0.018715057522058487\n",
            "iteration:2 step:23 loss:0.02737133763730526\n",
            "iteration:2 step:24 loss:0.011508301831781864\n",
            "iteration:2 step:25 loss:0.01946392096579075\n",
            "iteration:2 step:26 loss:0.016736073419451714\n",
            "iteration:2 step:27 loss:0.03036288358271122\n",
            "iteration:2 step:28 loss:0.04467743635177612\n",
            "iteration:2 step:29 loss:0.023491302505135536\n",
            "iteration:2 step:30 loss:0.013949546962976456\n",
            "iteration:2 step:31 loss:0.03127194941043854\n",
            "iteration:2 step:32 loss:0.02183946967124939\n",
            "iteration:2 step:33 loss:0.02446054294705391\n",
            "iteration:2 step:34 loss:0.05186091363430023\n",
            "iteration:2 step:35 loss:0.011064023710787296\n",
            "iteration:2 step:36 loss:0.06964030861854553\n",
            "iteration:2 step:37 loss:0.017374347895383835\n",
            "iteration:2 step:38 loss:0.02471754513680935\n",
            "iteration:2 step:39 loss:0.048243749886751175\n",
            "iteration:2 step:40 loss:0.059002771973609924\n",
            "iteration:2 step:41 loss:0.04080694541335106\n",
            "iteration:2 step:42 loss:0.009901127777993679\n",
            "iteration:2 step:43 loss:0.03256550058722496\n",
            "iteration:2 step:44 loss:0.04857763275504112\n",
            "iteration:2 step:45 loss:0.05441529303789139\n",
            "iteration:2 step:46 loss:0.04723452404141426\n",
            "iteration:2 step:47 loss:0.02096881903707981\n",
            "iteration:2 step:48 loss:0.03133601322770119\n",
            "iteration:2 step:49 loss:0.01070536021143198\n",
            "iteration:2 step:50 loss:0.012839466333389282\n",
            "iteration:2 step:51 loss:0.017423558980226517\n",
            "iteration:2 step:52 loss:0.021161992102861404\n",
            "iteration:2 step:53 loss:0.12335213273763657\n",
            "iteration:2 step:54 loss:0.06609897315502167\n",
            "iteration:2 step:55 loss:0.05527881160378456\n",
            "iteration:2 step:56 loss:0.0733262225985527\n",
            "iteration:2 step:57 loss:0.04632130265235901\n",
            "iteration:2 step:58 loss:0.025193121284246445\n",
            "iteration:2 step:59 loss:0.04955765977501869\n",
            "iteration:2 step:60 loss:0.03768709674477577\n",
            "iteration:2 step:61 loss:0.038151513785123825\n",
            "iteration:2 step:62 loss:0.037847958505153656\n",
            "iteration:2 step:63 loss:0.04962988942861557\n",
            "iteration:2 step:64 loss:0.1235770732164383\n",
            "iteration:2 step:65 loss:0.016866885125637054\n",
            "iteration:2 step:66 loss:0.04763970151543617\n",
            "iteration:2 step:67 loss:0.0214194618165493\n",
            "iteration:2 step:68 loss:0.06658873707056046\n",
            "iteration:2 step:69 loss:0.07188571989536285\n",
            "iteration:2 step:70 loss:0.04268686845898628\n",
            "iteration:2 step:71 loss:0.02934855967760086\n",
            "iteration:2 step:72 loss:0.07587871700525284\n",
            "iteration:2 step:73 loss:0.03957051783800125\n",
            "iteration:2 step:74 loss:0.023553727194666862\n",
            "iteration:2 step:75 loss:0.019996672868728638\n",
            "iteration:2 step:76 loss:0.02753884717822075\n",
            "iteration:2 step:77 loss:0.016498370096087456\n",
            "iteration:2 step:78 loss:0.02291933260858059\n",
            "iteration:2 step:79 loss:0.05846107378602028\n",
            "iteration:2 step:80 loss:0.0603778101503849\n",
            "iteration:2 step:81 loss:0.019582025706768036\n",
            "iteration:2 step:82 loss:0.0047074719332158566\n",
            "iteration:2 step:83 loss:0.014029283076524734\n",
            "iteration:2 step:84 loss:0.027275513857603073\n",
            "iteration:2 step:85 loss:0.05151519179344177\n",
            "iteration:2 step:86 loss:0.011678023263812065\n",
            "iteration:2 step:87 loss:0.04848180711269379\n",
            "iteration:2 step:88 loss:0.020571408793330193\n",
            "iteration:2 step:89 loss:0.010567196644842625\n",
            "iteration:2 step:90 loss:0.056402355432510376\n",
            "iteration:2 step:91 loss:0.05631250515580177\n",
            "iteration:2 step:92 loss:0.03525639697909355\n",
            "iteration:2 step:93 loss:0.06370623409748077\n",
            "iteration:2 step:94 loss:0.008009685203433037\n",
            "iteration:2 step:95 loss:0.02381087653338909\n",
            "iteration:2 step:96 loss:0.024885136634111404\n",
            "iteration:2 step:97 loss:0.021383320912718773\n",
            "iteration:2 step:98 loss:0.0837937444448471\n",
            "iteration:2 step:99 loss:0.05095551162958145\n",
            "iteration:2 step:100 loss:0.024505551904439926\n",
            "iteration:2 step:101 loss:0.02472010627388954\n",
            "iteration:2 step:102 loss:0.014804098755121231\n",
            "iteration:2 step:103 loss:0.012399069964885712\n",
            "iteration:2 step:104 loss:0.027532214298844337\n",
            "iteration:2 step:105 loss:0.026550637558102608\n",
            "iteration:2 step:106 loss:0.027490386739373207\n",
            "iteration:2 step:107 loss:0.022146383300423622\n",
            "iteration:2 step:108 loss:0.026189427822828293\n",
            "iteration:2 step:109 loss:0.0507194921374321\n",
            "iteration:2 step:110 loss:0.02307642251253128\n",
            "iteration:2 step:111 loss:0.036753736436367035\n",
            "iteration:2 step:112 loss:0.031920790672302246\n",
            "iteration:2 step:113 loss:0.042462971061468124\n",
            "iteration:2 step:114 loss:0.02370373345911503\n",
            "iteration:2 step:115 loss:0.04301714897155762\n",
            "iteration:2 step:116 loss:0.012079376727342606\n",
            "iteration:2 step:117 loss:0.021124832332134247\n",
            "iteration:2 step:118 loss:0.027627162635326385\n",
            "iteration:2 step:119 loss:0.04079234227538109\n",
            "iteration:2 step:120 loss:0.02661316655576229\n",
            "iteration:2 step:121 loss:0.029564034193754196\n",
            "iteration:2 step:122 loss:0.021550573408603668\n",
            "iteration:2 step:123 loss:0.09207415580749512\n",
            "iteration:2 step:124 loss:0.054943714290857315\n",
            "iteration:2 step:125 loss:0.032409653067588806\n",
            "iteration:2 step:126 loss:0.021867074072360992\n",
            "iteration:2 step:127 loss:0.034791249781847\n",
            "iteration:2 step:128 loss:0.032730165868997574\n",
            "iteration:2 step:129 loss:0.020115774124860764\n",
            "iteration:2 step:130 loss:0.06112748757004738\n",
            "iteration:2 step:131 loss:0.02178545668721199\n",
            "iteration:2 step:132 loss:0.042006369680166245\n",
            "iteration:2 step:133 loss:0.06088049337267876\n",
            "iteration:2 step:134 loss:0.04458938539028168\n",
            "iteration:2 step:135 loss:0.022773267701268196\n",
            "iteration:2 step:136 loss:0.02094448357820511\n",
            "iteration:2 step:137 loss:0.029515713453292847\n",
            "iteration:2 step:138 loss:0.08525530993938446\n",
            "iteration:2 step:139 loss:0.05045928433537483\n",
            "iteration:2 step:140 loss:0.03339804336428642\n",
            "iteration:2 step:141 loss:0.02813589572906494\n",
            "iteration:2 step:142 loss:0.019391819834709167\n",
            "iteration:2 step:143 loss:0.037271395325660706\n",
            "iteration:2 step:144 loss:0.02978047914803028\n",
            "iteration:2 step:145 loss:0.021921416744589806\n",
            "iteration:2 step:146 loss:0.022004088386893272\n",
            "iteration:2 step:147 loss:0.009814692661166191\n",
            "iteration:2 step:148 loss:0.01246999204158783\n",
            "iteration:2 step:149 loss:0.061025895178318024\n",
            "iteration:2 step:150 loss:0.01875345967710018\n",
            "iteration:2 step:151 loss:0.06319861114025116\n",
            "iteration:2 step:152 loss:0.060354407876729965\n",
            "iteration:2 step:153 loss:0.015427215956151485\n",
            "iteration:2 step:154 loss:0.024395011365413666\n",
            "iteration:2 step:155 loss:0.033868297934532166\n",
            "iteration:2 step:156 loss:0.04819709435105324\n",
            "iteration:2 step:157 loss:0.04294935613870621\n",
            "iteration:2 step:158 loss:0.0548209547996521\n",
            "iteration:2 step:159 loss:0.006134688854217529\n",
            "iteration:2 step:160 loss:0.019191544502973557\n",
            "iteration:2 step:161 loss:0.08185014873743057\n",
            "iteration:2 step:162 loss:0.067008376121521\n",
            "iteration:2 step:163 loss:0.0397367887198925\n",
            "iteration:2 step:164 loss:0.025127585977315903\n",
            "iteration:2 step:165 loss:0.02345079928636551\n",
            "iteration:2 step:166 loss:0.015083642676472664\n",
            "iteration:2 step:167 loss:0.026267478242516518\n",
            "iteration:2 step:168 loss:0.07653403282165527\n",
            "iteration:2 step:169 loss:0.0158026572316885\n",
            "iteration:2 step:170 loss:0.005617468152195215\n",
            "iteration:2 step:171 loss:0.04706266522407532\n",
            "iteration:2 step:172 loss:0.01719428412616253\n",
            "iteration:2 step:173 loss:0.05873914808034897\n",
            "iteration:2 step:174 loss:0.021533668041229248\n",
            "iteration:2 step:175 loss:0.04326338693499565\n",
            "iteration:2 step:176 loss:0.05429711937904358\n",
            "iteration:2 step:177 loss:0.022567296400666237\n",
            "iteration:2 step:178 loss:0.014798859134316444\n",
            "iteration:2 step:179 loss:0.018311601132154465\n",
            "iteration:2 step:180 loss:0.020304769277572632\n",
            "iteration:2 step:181 loss:0.03744315728545189\n",
            "iteration:2 step:182 loss:0.04233399033546448\n",
            "iteration:2 step:183 loss:0.018753519281744957\n",
            "iteration:2 step:184 loss:0.05331037938594818\n",
            "iteration:2 step:185 loss:0.030409425497055054\n",
            "iteration:2 step:186 loss:0.06835667043924332\n",
            "iteration:2 step:187 loss:0.042254216969013214\n",
            "iteration:2 step:188 loss:0.016048556193709373\n",
            "iteration:2 step:189 loss:0.03971686586737633\n",
            "iteration:2 step:190 loss:0.00349765014834702\n",
            "iteration:2 step:191 loss:0.03056989796459675\n",
            "iteration:2 step:192 loss:0.020781414583325386\n",
            "iteration:2 step:193 loss:0.023879574611783028\n",
            "iteration:2 step:194 loss:0.01617855206131935\n",
            "iteration:2 step:195 loss:0.00634898291900754\n",
            "iteration:2 step:196 loss:0.05668732151389122\n",
            "iteration:2 step:197 loss:0.03636547178030014\n",
            "iteration:2 step:198 loss:0.0041132764890789986\n",
            "iteration:2 step:199 loss:0.0881209522485733\n",
            "iteration:2 step:200 loss:0.02430330403149128\n",
            "iteration:2 step:201 loss:0.04140353947877884\n",
            "iteration:2 step:202 loss:0.02052905410528183\n",
            "iteration:2 step:203 loss:0.02378242276608944\n",
            "iteration:2 step:204 loss:0.015308835543692112\n",
            "iteration:2 step:205 loss:0.03774510696530342\n",
            "iteration:2 step:206 loss:0.0514378659427166\n",
            "iteration:2 step:207 loss:0.11539248377084732\n",
            "iteration:2 step:208 loss:0.12606674432754517\n",
            "iteration:2 step:209 loss:0.03403449431061745\n",
            "iteration:2 step:210 loss:0.050043266266584396\n",
            "iteration:2 step:211 loss:0.01275752205401659\n",
            "iteration:2 step:212 loss:0.11142544448375702\n",
            "iteration:2 step:213 loss:0.00845388974994421\n",
            "iteration:2 step:214 loss:0.0411812998354435\n",
            "iteration:2 step:215 loss:0.029046563431620598\n",
            "iteration:2 step:216 loss:0.03624477609992027\n",
            "iteration:2 step:217 loss:0.10182197391986847\n",
            "iteration:2 step:218 loss:0.01911456324160099\n",
            "iteration:2 step:219 loss:0.021635640412569046\n",
            "iteration:2 step:220 loss:0.03199184685945511\n",
            "iteration:2 step:221 loss:0.04814259707927704\n",
            "iteration:2 step:222 loss:0.0143339978531003\n",
            "iteration:2 step:223 loss:0.027624810114502907\n",
            "iteration:2 step:224 loss:0.042948681861162186\n",
            "iteration:2 step:225 loss:0.016908057034015656\n",
            "iteration:2 step:226 loss:0.05653688311576843\n",
            "iteration:2 step:227 loss:0.027603542432188988\n",
            "iteration:2 step:228 loss:0.055451709777116776\n",
            "iteration:2 step:229 loss:0.038519732654094696\n",
            "iteration:2 step:230 loss:0.02817836031317711\n",
            "iteration:2 step:231 loss:0.025207607075572014\n",
            "iteration:2 step:232 loss:0.025575438514351845\n",
            "iteration:2 step:233 loss:0.0361386276781559\n",
            "iteration:2 step:234 loss:0.03262785077095032\n",
            "iteration:2 step:235 loss:0.03471825271844864\n",
            "iteration:2 step:236 loss:0.013552910648286343\n",
            "iteration:2 step:237 loss:0.02151022106409073\n",
            "iteration:2 step:238 loss:0.03079342283308506\n",
            "iteration:2 step:239 loss:0.03903103247284889\n",
            "iteration:2 step:240 loss:0.024109063670039177\n",
            "iteration:2 step:241 loss:0.02990284375846386\n",
            "iteration:2 step:242 loss:0.020975330844521523\n",
            "iteration:2 step:243 loss:0.09280950576066971\n",
            "iteration:2 step:244 loss:0.02398804761469364\n",
            "iteration:2 step:245 loss:0.03026164136826992\n",
            "iteration:2 step:246 loss:0.046283043920993805\n",
            "iteration:2 step:247 loss:0.04300132766366005\n",
            "iteration:2 step:248 loss:0.041449837386608124\n",
            "iteration:2 step:249 loss:0.033992212265729904\n",
            "iteration:2 step:250 loss:0.047359056770801544\n",
            "iteration:2 step:251 loss:0.03653542324900627\n",
            "iteration:2 step:252 loss:0.039121147245168686\n",
            "iteration:2 step:253 loss:0.057608749717473984\n",
            "iteration:2 step:254 loss:0.027740374207496643\n",
            "iteration:2 step:255 loss:0.047635287046432495\n",
            "iteration:2 step:256 loss:0.023243743926286697\n",
            "iteration:2 step:257 loss:0.009888127446174622\n",
            "iteration:2 step:258 loss:0.019724341109395027\n",
            "iteration:2 step:259 loss:0.01740514300763607\n",
            "iteration:2 step:260 loss:0.056313127279281616\n",
            "iteration:2 step:261 loss:0.059261880815029144\n",
            "iteration:2 step:262 loss:0.016243817284703255\n",
            "iteration:2 step:263 loss:0.032653965055942535\n",
            "iteration:2 step:264 loss:0.008083086460828781\n",
            "iteration:2 step:265 loss:0.013637200929224491\n",
            "iteration:2 step:266 loss:0.03933858871459961\n",
            "iteration:2 step:267 loss:0.007799079176038504\n",
            "iteration:2 step:268 loss:0.08710115402936935\n",
            "iteration:2 step:269 loss:0.02895977720618248\n",
            "iteration:2 step:270 loss:0.04728736728429794\n",
            "iteration:2 step:271 loss:0.08064690977334976\n",
            "iteration:2 step:272 loss:0.04265972226858139\n",
            "iteration:2 step:273 loss:0.03243902698159218\n",
            "iteration:2 step:274 loss:0.024445895105600357\n",
            "iteration:2 step:275 loss:0.050245415419340134\n",
            "iteration:2 step:276 loss:0.029015270993113518\n",
            "iteration:2 step:277 loss:0.07359206676483154\n",
            "iteration:2 step:278 loss:0.06510241329669952\n",
            "iteration:2 step:279 loss:0.010667209513485432\n",
            "iteration:2 step:280 loss:0.024553999304771423\n",
            "iteration:2 step:281 loss:0.050545305013656616\n",
            "iteration:2 step:282 loss:0.09307022392749786\n",
            "iteration:2 step:283 loss:0.01647389680147171\n",
            "iteration:2 step:284 loss:0.057672251015901566\n",
            "iteration:2 step:285 loss:0.007220654748380184\n",
            "iteration:2 step:286 loss:0.013301094993948936\n",
            "iteration:2 step:287 loss:0.03829670324921608\n",
            "iteration:2 step:288 loss:0.02626361884176731\n",
            "iteration:2 step:289 loss:0.08061356842517853\n",
            "iteration:2 step:290 loss:0.017366444692015648\n",
            "iteration:2 step:291 loss:0.025590065866708755\n",
            "iteration:2 step:292 loss:0.06869038939476013\n",
            "iteration:2 step:293 loss:0.012708581052720547\n",
            "iteration:2 step:294 loss:0.025986025109887123\n",
            "iteration:2 step:295 loss:0.08990718424320221\n",
            "iteration:2 step:296 loss:0.01183638907968998\n",
            "iteration:2 step:297 loss:0.014162351377308369\n",
            "iteration:2 step:298 loss:0.025480961427092552\n",
            "iteration:2 step:299 loss:0.06792349368333817\n",
            "iteration:2 step:300 loss:0.026212433353066444\n",
            "iteration:2 step:301 loss:0.04392905905842781\n",
            "iteration:2 step:302 loss:0.03457353636622429\n",
            "iteration:2 step:303 loss:0.005712709855288267\n",
            "iteration:2 step:304 loss:0.04116611182689667\n",
            "iteration:2 step:305 loss:0.008152863010764122\n",
            "iteration:2 step:306 loss:0.03277929499745369\n",
            "iteration:2 step:307 loss:0.13080379366874695\n",
            "iteration:2 step:308 loss:0.024880370125174522\n",
            "iteration:2 step:309 loss:0.022494975477457047\n",
            "iteration:2 step:310 loss:0.030880901962518692\n",
            "iteration:2 step:311 loss:0.024571655318140984\n",
            "iteration:2 step:312 loss:0.026256518438458443\n",
            "iteration:2 step:313 loss:0.036273472011089325\n",
            "iteration:2 step:314 loss:0.037386007606983185\n",
            "iteration:2 step:315 loss:0.02232912741601467\n",
            "iteration:2 step:316 loss:0.008682381361722946\n",
            "iteration:2 step:317 loss:0.01902957819402218\n",
            "iteration:2 step:318 loss:0.04561672359704971\n",
            "iteration:2 step:319 loss:0.02794455736875534\n",
            "iteration:2 step:320 loss:0.04502065107226372\n",
            "iteration:2 step:321 loss:0.045462265610694885\n",
            "iteration:2 step:322 loss:0.053145065903663635\n",
            "iteration:2 step:323 loss:0.034323420375585556\n",
            "iteration:2 step:324 loss:0.061735279858112335\n",
            "iteration:2 step:325 loss:0.017586899921298027\n",
            "iteration:2 step:326 loss:0.016706237569451332\n",
            "iteration:2 step:327 loss:0.04788787290453911\n",
            "iteration:2 step:328 loss:0.016866499558091164\n",
            "iteration:2 step:329 loss:0.02182948775589466\n",
            "iteration:2 step:330 loss:0.039029039442539215\n",
            "iteration:2 step:331 loss:0.035339225083589554\n",
            "iteration:2 step:332 loss:0.07476603984832764\n",
            "iteration:2 step:333 loss:0.019218867644667625\n",
            "iteration:2 step:334 loss:0.043787989765405655\n",
            "iteration:2 step:335 loss:0.05115080997347832\n",
            "iteration:2 step:336 loss:0.08792445063591003\n",
            "iteration:2 step:337 loss:0.017385920509696007\n",
            "iteration:2 step:338 loss:0.010546436533331871\n",
            "iteration:2 step:339 loss:0.02732829563319683\n",
            "iteration:2 step:340 loss:0.036724139004945755\n",
            "iteration:2 step:341 loss:0.046836160123348236\n",
            "iteration:2 step:342 loss:0.04465233534574509\n",
            "iteration:2 step:343 loss:0.031752634793519974\n",
            "iteration:2 step:344 loss:0.04925982654094696\n",
            "iteration:2 step:345 loss:0.012068324722349644\n",
            "iteration:2 step:346 loss:0.03935438022017479\n",
            "iteration:2 step:347 loss:0.0747729167342186\n",
            "iteration:2 step:348 loss:0.027104025706648827\n",
            "iteration:2 step:349 loss:0.01628335379064083\n",
            "iteration:2 step:350 loss:0.030791282653808594\n",
            "iteration:2 step:351 loss:0.021158620715141296\n",
            "iteration:2 step:352 loss:0.04239916801452637\n",
            "iteration:2 step:353 loss:0.03324436396360397\n",
            "iteration:2 step:354 loss:0.03179012984037399\n",
            "iteration:2 step:355 loss:0.04858214035630226\n",
            "iteration:2 step:356 loss:0.04747876897454262\n",
            "iteration:2 step:357 loss:0.018237486481666565\n",
            "iteration:2 step:358 loss:0.053765587508678436\n",
            "iteration:2 step:359 loss:0.05224229767918587\n",
            "iteration:2 step:360 loss:0.029894694685935974\n",
            "iteration:2 step:361 loss:0.09219320863485336\n",
            "iteration:2 step:362 loss:0.027533287182450294\n",
            "iteration:2 step:363 loss:0.011629793792963028\n",
            "iteration:2 step:364 loss:0.01203942485153675\n",
            "iteration:2 step:365 loss:0.06895855069160461\n",
            "iteration:2 step:366 loss:0.024509882554411888\n",
            "iteration:2 step:367 loss:0.0672699436545372\n",
            "iteration:2 step:368 loss:0.018805021420121193\n",
            "iteration:2 step:369 loss:0.07418287545442581\n",
            "iteration:2 step:370 loss:0.05137031152844429\n",
            "iteration:2 step:371 loss:0.10632409900426865\n",
            "iteration:2 step:372 loss:0.042364902794361115\n",
            "iteration:2 step:373 loss:0.019218066707253456\n",
            "iteration:2 step:374 loss:0.06027306988835335\n",
            "iteration:2 step:375 loss:0.03366471454501152\n",
            "iteration:2 step:376 loss:0.009999698027968407\n",
            "iteration:2 step:377 loss:0.0236812811344862\n",
            "iteration:2 step:378 loss:0.013858907856047153\n",
            "iteration:2 step:379 loss:0.025487979874014854\n",
            "iteration:2 step:380 loss:0.01826702617108822\n",
            "iteration:2 step:381 loss:0.03106795810163021\n",
            "iteration:2 step:382 loss:0.11197885870933533\n",
            "iteration:2 step:383 loss:0.025083670392632484\n",
            "iteration:2 step:384 loss:0.028649456799030304\n",
            "iteration:2 step:385 loss:0.013642404228448868\n",
            "iteration:2 step:386 loss:0.050572022795677185\n",
            "iteration:2 step:387 loss:0.0645296573638916\n",
            "iteration:2 step:388 loss:0.01870497316122055\n",
            "iteration:2 step:389 loss:0.07460697740316391\n",
            "iteration:2 step:390 loss:0.02186277136206627\n",
            "iteration:2 step:391 loss:0.012980516999959946\n",
            "iteration:2 step:392 loss:0.05482659488916397\n",
            "iteration:2 step:393 loss:0.0671706572175026\n",
            "iteration:2 step:394 loss:0.04346771538257599\n",
            "iteration:2 step:395 loss:0.01780824176967144\n",
            "iteration:2 step:396 loss:0.021783459931612015\n",
            "iteration:2 step:397 loss:0.031626343727111816\n",
            "iteration:2 step:398 loss:0.014123057946562767\n",
            "iteration:2 step:399 loss:0.022351406514644623\n",
            "iteration:2 step:400 loss:0.06584222614765167\n",
            "iteration:2 step:401 loss:0.02648300677537918\n",
            "iteration:2 step:402 loss:0.021168891340494156\n",
            "iteration:2 step:403 loss:0.026055296882987022\n",
            "iteration:2 step:404 loss:0.0222261194139719\n",
            "iteration:2 step:405 loss:0.02423645183444023\n",
            "iteration:2 step:406 loss:0.038542844355106354\n",
            "iteration:2 step:407 loss:0.039689015597105026\n",
            "iteration:2 step:408 loss:0.03694647550582886\n",
            "iteration:2 step:409 loss:0.025180712342262268\n",
            "iteration:2 step:410 loss:0.011128507554531097\n",
            "iteration:2 step:411 loss:0.017814181745052338\n",
            "iteration:2 step:412 loss:0.04653344303369522\n",
            "iteration:2 step:413 loss:0.054984405636787415\n",
            "iteration:2 step:414 loss:0.031145384535193443\n",
            "iteration:2 step:415 loss:0.025561681017279625\n",
            "iteration:2 step:416 loss:0.004994553979486227\n",
            "iteration:2 step:417 loss:0.032665930688381195\n",
            "iteration:2 step:418 loss:0.02343229204416275\n",
            "iteration:2 step:419 loss:0.04666099697351456\n",
            "iteration:2 step:420 loss:0.009076729416847229\n",
            "iteration:2 step:421 loss:0.015028346329927444\n",
            "iteration:2 step:422 loss:0.03306707739830017\n",
            "iteration:2 step:423 loss:0.02602902054786682\n",
            "iteration:2 step:424 loss:0.013630671426653862\n",
            "iteration:2 step:425 loss:0.020465724170207977\n",
            "iteration:2 step:426 loss:0.0375257171690464\n",
            "iteration:2 step:427 loss:0.006981476675719023\n",
            "iteration:2 step:428 loss:0.04430026188492775\n",
            "iteration:2 step:429 loss:0.024249708279967308\n",
            "iteration:2 step:430 loss:0.006573857739567757\n",
            "iteration:2 step:431 loss:0.04448666423559189\n",
            "iteration:2 step:432 loss:0.06266232579946518\n",
            "iteration:2 step:433 loss:0.02514982782304287\n",
            "iteration:2 step:434 loss:0.017140841111540794\n",
            "iteration:2 step:435 loss:0.03532338887453079\n",
            "iteration:2 step:436 loss:0.026996418833732605\n",
            "iteration:2 step:437 loss:0.015462508425116539\n",
            "iteration:2 step:438 loss:0.026301706209778786\n",
            "iteration:2 step:439 loss:0.025726117193698883\n",
            "iteration:2 step:440 loss:0.02414667047560215\n",
            "iteration:2 step:441 loss:0.023092936724424362\n",
            "iteration:2 step:442 loss:0.02928183414041996\n",
            "iteration:2 step:443 loss:0.02589522674679756\n",
            "iteration:2 step:444 loss:0.009689691476523876\n",
            "iteration:2 step:445 loss:0.021084826439619064\n",
            "iteration:2 step:446 loss:0.01019570417702198\n",
            "iteration:2 step:447 loss:0.029023049399256706\n",
            "iteration:2 step:448 loss:0.02466960810124874\n",
            "iteration:2 step:449 loss:0.02636081352829933\n",
            "iteration:2 step:450 loss:0.02196650207042694\n",
            "iteration:2 step:451 loss:0.03285718709230423\n",
            "iteration:2 step:452 loss:0.010840365663170815\n",
            "iteration:2 step:453 loss:0.041409533470869064\n",
            "iteration:2 step:454 loss:0.008398839272558689\n",
            "iteration:2 step:455 loss:0.006548379547894001\n",
            "iteration:2 step:456 loss:0.01004946231842041\n",
            "iteration:2 step:457 loss:0.007653522305190563\n",
            "iteration:2 step:458 loss:0.010374864563345909\n",
            "iteration:2 step:459 loss:0.02128082886338234\n",
            "iteration:2 step:460 loss:0.0018297076458111405\n",
            "iteration:2 step:461 loss:0.003342034760862589\n",
            "iteration:2 step:462 loss:0.003760657273232937\n",
            "iteration:2 step:463 loss:0.0508662685751915\n",
            "iteration:2 step:464 loss:0.01727513037621975\n",
            "iteration:2 step:465 loss:0.005305640399456024\n",
            "iteration:2 step:466 loss:0.13109374046325684\n",
            "iteration:2 step:467 loss:0.004832716193050146\n",
            "iteration:2 step:468 loss:0.10017602890729904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  n_samples=0\n",
        "  n_correct=0\n",
        "  for images,labels in test_loader:\n",
        "    images=images.reshape(-1,28*28).to(device)\n",
        "    labels=labels.to(device)\n",
        "    outputs=model(images)\n",
        "    _,predicted=torch.max(outputs.data,1)\n",
        "    n_correct+=(predicted==labels).sum().item()\n",
        "    n_samples+=label.size(0)\n",
        "accuracy=(n_correct*100)/n_samples\n",
        "print(accuracy)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVPf0SWz7H9M",
        "outputId": "7f56e060-ff65-4b7c-9036-945fca167298"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "96.60799050632912\n"
          ]
        }
      ]
    }
  ]
}